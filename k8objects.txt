Format of pod manifest

File_name:  pod1.yaml

kind: Pod                       #kind is "Pod" for creating pod                 
apiVersion: v1                  #Api version for interacting with the K8S             
metadata:                       #Metadata for the pod               
  name: testpod                 #Name of the pod       
spec:                                    
  containers:                      
    - name: c00                 #Container name                
      image: ubuntu             #Image name            
      command: ["/bin/bash", "-c", "while true; do echo Hello Sanjay; sleep 5 ; done"]
  restartPolicy: Never          # Defaults to Always

Annotations
- Annotations is used for adding descriptions to the pod
- Annotations are defined in the metadata section 
- specified in the manifest
- Example
--------------------------------------------------------------------------------------------------------
kind: Pod                              
apiVersion: v1                     
metadata:                           
  name: testpod  
  annotations:
    description: This is first K8S pod (annotations)                
spec:                                    
  containers:                      
    - name: c00                     
      image: ubuntu              
      command: ["/bin/bash", "-c", "while true; do echo Hello Sanjay; sleep 5 ; done"]
  restartPolicy: Never         # Defaults to Always
--------------------------------------------------------------------------------------------------------
Multicontainer Pod
--------------------------------------------------------------------------------------------------------
- Multiple containers can be created in a pod; however, it is recommended to create one container/pod
- Even for mulitple containers per pod, one ip address will be assigned to the pod. 
- Internal communication between the containers can be done using "http://localhost:<port-no>" command
- refer to the "multicontainerPod.yaml" manifest for example

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Defining Environment Variables in pods
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- it is used to set some Environment variables for the pod
- useful for activities like passing secrets to the pod
- For pods, environement variables are defined in the spec section with "env:" keyword followed by key value pair.
- We can define any number of environement variables
- Example for the same is in "EnvVarPod.yaml" maintest

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Defining open port on the pods
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- We can open ports in the spec section  with keyword "- containerPort:<port-no>"
- Useful for opening the ports for required applications 
- We will try with example of Tomcat Server by opening port no 80

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Labels and Selectors
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- Labels are the mechanisms you use to Organize Kubernetes objects
- A label is a key-value pair without any predefined meaning that can be attached to a object
- Labels are similar to tags in AWS or git where you use a name for quick reference.
- You are free to choose labels as you need it to refer an environement which is used for dev or testing 
  or production, refer a product group like Department A, Department B.

- Multiple Lables can be given to a single object
- Labels are defined in the metadata section of the manifest
- example is given in the podTomcat.yaml and podFirst.yaml manifest
- kubectl commands for Labels 
    - labels can be seen using below commands
        - kubectl get pods --show-labels 
    - adding new labels to an existing pod
        - kubectl label pods tomcat cloud=false
    - listing pods matching a label
        - kubectl get pods -l env=dev  
    - listing condition can be defined as not equal as well 
        - kubectl get pods -l env!=dev
    - other operations also can be performed based on list condition
        - kubectl delete pod -l env!=dev   
        above command can delete multiple containers.
- Unlike name/UIDs, lables do not provide uniqueness, as in general, 
  we can expect many objects to carry the same label

- Once labels are attached to an object, we would need filters to narrow down
  these are called label selectors.
- The api currently supports two types of selectors 
    - equality based (= and !=)
        example env!=prod
    - set based (in, notin and exists)
        'env in (dev,prod,stage)'
        'env notin (dev,stage)'
- A label selector can be made of multiple requirements which are comma seperated
- mutiple values also can be matched together
    - kubectl get pods -l env=dev, app=tomcat, cloud=true 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Node Selector (a use case for Label selector)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
One use case for Label Selector can be is to constrain the set of nodes on to which a 
pod can be scheduled. 
 - Generally such constraints are not required because the scheduler will do the 
   reasonable placement on the cluster; however in some cases we might need it.
 - we can use labels to tag nodes.
 - Once the nodes are tagged, you can use the label selectors to specify the pod to 
   run only at specific nodes.

 - node selector is defined in the spec section with "nodeSelector:" keyword
 - The object (pod) will not be created until the condition is met on the cluster.
 - kubectl get pods commands will show the status as pending.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Scaling and Replication
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- Kubernetes was designed to orchestrate multiple containers and replication
- Need for multiple containers/replication helps us with these.
- Reliablity: It help in getting high reliablity; in case any container fails, application will not
  get down.
- Load Balancing: Having mulitple versions of a containers enables you to easily send traffic to different
  instances to prevent overloading of a single instance or node.
- Scaling : When load does become too much for the number of existing instances,
  Kubernetes enables you to easily scale up/down your application, adding additional instances
  as needed.
- Rolling Updates: Updates to a service by replacing podes one by one.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Replication Controller (rc) 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- A replication controller is a object that enables you to easily create multiple pods on request, then make sure
  that number of pods always exist.
- If a pod created using RC will be automatically replaced in case of crash/fail or termination action.
- RC is recommended if you just want to make sure that fix number of pods are always running, 
  even after reboot of the instance.
- In case you create RC with 1 replica & the RC will make sure that one pod is always 
  running.
- While scaling down K8S delete the most recent containers 
- Please refer to the "rcUbuntu.yaml" for Replication Controller Manifest example

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Replication Set (rs) 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- A replica Set is next generation Replication Controller. 
- replication Controller only supports equality based selector, whereas the replica set supports set-based 
  selector
- replicaset rather than replication controller is used by other objects like Deployment
- difference in the manifest
    Replication Controller                                     replicaset
  kind: ReplicationController                           kind: Replicaset
  apiVersion: v1                                        apiVersion: apps/v1
  selector:                                             selector:
    env: dev                                            env in (dev, staging, prod)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Difference b/w replicaset and replication Controller
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
1. Replicase Set have a "selector" section in the spec section which allows replicaset to manage
   existing running pods.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Ways to Scale the pods in the cluster using replicaset
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
1. change the manifest file and use replace command:
   kubectl replace -f replicaset-definition.yaml 
2. by defining replicas in the kubectl command
   kubectl scale --replicas=6 -f replicaset-definition.yaml 
                    or
   kubectl scale --replicas=6 replicaset myapp-replicaset
                              --type---- ----name--------

  The option 2 will not increase the number of replicas in the manifest file.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Commands:
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Create the replicaset
  kubectl create -f replicaset-definition.yaml
get the replicasets from the cluster
  kubectl get replicaset
delete the replicaset
  kubectl delete replicaset myapp-replicaset  (this will delete all underlying pods)
scale the replicaset pods
   kubectl scale --replicas=6 -f replicaset-definition.yaml 
                    or
   kubectl scale --replicas=6 replicaset myapp-replicaset
                              --type---- ----name--------
Editing the existing running replicaset
   Kubectl edit replicaset myapp-replicaset 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Using Run Command to create the yaml files
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Kubernetes (K8s) Deployment Objects
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Replication Controller and replicaset is not able to do the updates and rollback apps in the cluster.
- A deployment object acts as a supervisor for pods, giving you fine grained control over how and when a new
  pod is rolled out, updated or rolled back to a previous state.
- When using deployment object, we first define the state of the app, then K8s cluster schedules mentioned app
  instance onto specific individual nodes.
- A deployment provides declarative updates for pods & replicaset.
- K8s then monitors, if the node hosting an instance goes down or pod is deleted; the deployment controller 
  replaces it.
- This provides a self-healing mechanism to address machine failure or maintenance.
- Below are the typical use cases of deployment:
    1. Create a deployment to rollout a replicaset 
       The replicaset creates pods in the background and checks the status of the rollout to see if it succeeds or not.
    2. Declare new state of the pods -> 
       By updating the podTemplateSpec of the deployment, a new replicaset is created and the deployment manages moving the pods from the old replicaset to the new
       one at a controlled rate. Each new replicaset updates the revision of the deployment.
    3. Rollback to an earlier deployment revision. If the current state of the deployment is not stable, each rollback updates
       the revision of the deployment.
    4. Scale up the deployment to facilitates more load
    5. Pause the deployment to apply multiple fixes to its podTemplateSpec and then resume it to start a new rollout.
    6. Cleanup older replicasets that you don't need anymore.
- If there are problems in the deployment, Kubernetes will automatically rollback to the previous version; however, you can also 
  explicitely rollback to a specific revision, as in our case to revision 1 (the original pod version)
- You can rollback to a specific version by specifying it with --to-revision

for example: kubectl rollout undo deploy/mydeployments --to-revision=2
Note: The name of the replicaset is always formatted as [deployment-name]-[random-string]

"kubectl get deploy" command can be used to get the deployments.
While inspecting the deployments, we can see the below fields: 
NAME - List the name of the deployments in the namespace.
READY - Display how many replicas of the application are available to your users. If follows the pattern ready/Desired.
UP-TO-DATE - Display the number of replicas that have been updated to achive the desired state.
AVAILABLE - Displays how many replicas of the application are available to your users.
AGE - Displays the amount of the time that the application has been running.

Reasons for failed deployments:
Your deployment may get stuck trying to deploy its newest replicaset without ever completing.
This can occur due to some of the following factors:
1. Insufficient Quota
2. Readiness Proble Failure
3. Image pull errors
4. Insufficient permissions
5. Limit Ranges
6. Application runtime misconfiguration




+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Service Object
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Service object allows to provision communication between differnt networks on the Kubernetes.

Spec Section of the Service Definition file is different:

spec:
    type: NodePort
    ports: 
     - targetPort: 80
       port: 80
       nodePort:30008
* only port is the mandatory field for service definition; others will be assumed as under:
  - NodePort will be allocated based on availablity from 30000-32767
  - targetPort will be assumed as port

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Differenece b/w Create and apply command
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Create command is an imperative command for applying changes to the cluster.

However, Apply command is declarative method of applying the changes to the infrastructure configuration.
- It creates a json file for last applied configuration which is checked everytime before applying the 
  configuration to the cluster.
- For example, if there is a change in the local manifest file and the same change is not in live env
  and is not in the last applied configuration. The change will be applied.
- For removing some changes from the live cluster, the change should exist in the last applied configuration
  and should not exist in the local manifest file.
- We should not mix the declarative and imperative commands while working with K8S cluster.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Labels and Selectors
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Labels are used to identify the K8s objects based on certain criteria
  - Selector keyword is used to group the K8s Objects with Kubectl commands.
  - Labels and selectors are used for the connecting internally with in K8s as well
    e.g. Deployment and replicasets are connected with labels and selectors only.
         Replicaset and pods are connected in the same way

    commands for selecting the objects with labels
     kubectl get pods --selector env=dev | wc -l

     count the pods with particular labels
     kubectl get pods --selector env=dev | wc -l

     Removing header before displaying the pods
     kubectl get pods --selector env=dev --no-headers | wc -l

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Taints and Tolerations
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Taints:
 - It is a property applied on the nodes to restrict them for accepting pods with applied tolerations.
 - By default node is able to accept all kinds of pods
 - If we want pod to be able to create on certain node, we need to define toleration on the pod
 - there are three types of taints for the node:
    - NoSchedule - Don't schedule pod on the node 
    - PreferNoSchedule - Prefer not to schedule but there is no guarntee that pod will not be scheduled 
    - NoExecute - This is usually applied when already pods are running on this node. The effect will
                  kill the pods which don't have tolerations and pods with toleration will have no impact.

- command for applying taint on the node
    kubectl taint nodes node-name key=value:taint-effect

    kubectl taint nodes node1 app=blue:NoSchedule

- applying tolerations in the pods
  spec:
    tolerations:
    - key: "app"
      operator: "Equal"
      value: "blue"
      effect: "NoSchedule"
    THE VALUES FOR THE TOLERATIONS WILL BE IN DOUBLE QUOTES ("").

- Checking whether node is tainted or not
  kubectl describe node <node-name> | grep Taints 

- Normally master node is tainted with NoSchedule; however, it can be changed if required.
- Untainting the master node can be done with below command:
  kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule-
- Tolerations allow pod to be created on tainted node; however, a pod with toleration can be created on 
  non-tainted nodes. So toleration and tainting doesn't guarntee that a pod will be created on certain node only.
- Tolerations and tainting have nothing to do with security.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Node Selectors
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

- In case we want to place our node of some particular node, we can define the same using the below 
  approach:
  1. Assign a label to the node 
  2. Use the label in the Pod Definition File for selecting the node for provisioning the pod.

- Example:
Labelling the node with size=large
  Kubectl label nodes <node-name> <label-key>=<label-value>
  Kubectl label nodes node1 size=large

Assigning node selector in the pod definition file
  spec:
     nodeSelector: 
         size: large

This can solve the problem of provisioing the pod on some particular node; however, this can't solve the problem 
of provisioing the pod on complex conditions (Not or OR conditions) e.g. large or medium node/ not small 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Node Affinity
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
The complex conditions for the pod provisioing on the node can be handled using node affinity. 
- Node Affinity is assigned within the node specification file
e.g.

spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoreDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - large
            - medium

  other values for operator can be:
   notIn, Exists 

   Exists can be used in case the label only exists irrespective of their value.
   e.g. we have not provisioned small size nodes. So using Exist operator without values should place
        the pod on either of the nodes.

  Node Affinity Types:
  
  available Affinity types
  requiredDuringSchedulingIgnoredDuringExecution
  preferredDuringSchedulingIgnoredDuringExecution
  -------------------------------------------------------------------------------
  |                       | During Schduling  |  During Execution               |
  -------------------------------------------------------------------------------
  |    Type 1             |  Required         |   Ignored                       |
  -------------------------------------------------------------------------------
  |    Type 2             |  Preferred        |   Ignored                       |
  -------------------------------------------------------------------------------
  For the above two types the Node Affinity will be considered only while scheduling the pods.
  Not during Execution.

  The below is planned:
  requiredDuringSchedulingRequiredDuringExecution
  -------------------------------------------------------------------------------
  |                       | During Schduling  |  During Execution               |
  -------------------------------------------------------------------------------
  |    Type 3             |  Required         |   Required                      |
  -------------------------------------------------------------------------------
  In this case if the label is removed after scheduling the pod, the pod will be killed.

  For keeping the pods on required nodes, sometime a mixture of Node Affinity, taint and toleration both are 
  used in conjunction.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Resource Limits
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

By default, K8s looks for 0.5 cpu and 256 Mi of memory for a new pod creation request;however, this can be 
changed in the pod definition file as shown under:

spec:
  containers:
  - name: nginx
    image: nginx
    ports:
     - containerPort:8080
  resources:
    requests:
      memory: "1Gi"
      cpu: 1
    limit:
      memory:"2Gi"
      cpu: 2

similary by default, the kubernetes will limit the cpu & memory limits for the pod as 1 vcpu and 512 Mi; However, it can 
be defined in the pod definition file if you need over the default requirement.

**INCASE THE POD NEEDS MORE CPU THAN DEFINED, K8S WILL THROTTLE THE CPU. HOWEVER, POD MAY BE USING MORE MEMORY THAN Required
HOWEVER, IF IT HAPPENS FREQUENTLY, THE POD WILL BE TERMINATED.


                            Different units for the memory:           
1G (Gigabyte) = 1,000,000,000 bytes       | 1 Gi (Gibibyte) = 1,073,741,824 bytes
1M (Megabyte) = 1,000,000 bytes           | 1 Mi (Mebibyte) = 1,048,576 bytes
1K (Kilo)     = 1,000 bytes               | 1 Ki (Kibibyte) = 1,024 Bytes


When a pod is created the containers are assigned a default CPU request of .5 and memory of 256Mi". 
For the POD to pick up those defaults you must have first set those as default values for request and 
limit by creating a LimitRange in that namespace.

apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/

apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/

https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource
  
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
DaemonSets
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- Daemon set is a K8S objects used for scheduling pod on each node.
- Use Cases:
  1. Installing Moniotring Solution of each node
  2. Implementing Networking Solutions
  3. Installing Kube Proxy (a required Element on the Node)
- Daemonsets are similar to ReplicaSets in K8S
- Daemonsets create a pod on each node as the node is created in the network.
- How to implement the creation of DaemonPods on each node.
    - Before Kubernetes v1.12, the pod were created with the nodeName. So the pod land on the required node.
    - After v1.12, session affinity and default scheduler is used for creating pods on the nodes.

Steps for creating the Daemonsets
1. Get the YAML file for creating the Deployment using the below command:
   kubectl create deployment <deplyment-name> --image=<image-name> -n <name-space-name> --dry-run=client
   -o yaml > Daemon.yaml

2. Remove the below configurations from the yaml file
    - status
    - Stratergy
    - replicas
   replace the below configuration from the yaml file
    - deplyment keyword with DaemonSet keyword

3. Run the the manifest
  
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Static Pods
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Finding the Static Pod path for the node

1. Find the config file
   ps -aux | grep kubelet
   search --config (--config=/var/lib/kubelet/config.yaml)
2. Find the "staticPodPath" in the config file.
   staticPodPath: /etc/kubernetes/manifests

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mulitple Schedulers
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Multiple Schedulers can be run on the Kubernetes Master node by creating a copy of the 
default scheduler or creating your own.
Steps for spinning the additional Scheduler are as under:
1. Copy the /etc/kubernetes/manifests/kube-scheduler.yaml file to new-name
2. open the file in a editor
3. add scheduler name in the command section
4. define lock-object=<scheduler-name>
5. Create the pod using Kubectl Create Command

Viewing Schedulers
-------------------
kubectl get pods -n kube-system

The new scheduler should appear here and should be in running status.

Assigning scheduler to the pod
------------------------------
The pod definition file will have new field named schedulerName: and the new scheduler name.


Vieving events
---------------
kubectl get events -o wide

This command should show which scheduler picked up the pod creation with Source of the event.

Kubectl logs <custom-scheduler-name> --namespace=kube-system

The above command will show the logs for the scheduler.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Monitoring the Cluster
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
K8S cluster can be monitored using below tools for cpu and memory usage:
1. Metrics Server   - In memory monitoring solution and can't be used for historical 
                      performance analysis.
2. Prometheus  
3. Elastic stack
4. Datadog 
5. Dynatrace 

* one of the earlies project for the K8S monitoring was HEAPSTER; however, it is not deprecated

Metric Server is one of the good options available now. 
- You can have one meteric server per K8S cluster
- It is an Inmemory store
- Performance Data is collected from Kubelet (aAdvisor) 

For Minikube use the below command to enable the Metrics-server
minikube addons enable metrics-server

For all others
Need to clone the repo from github for meterics server and creating the required components.

Common Commands for the Performance Check:

kubectl top node
kubectl top pod

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Kubernetes Logging
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Logs for the Kubernetes pods are inspected using the below command
when one container per pod 
  kubectl logs -f <pod-name>
When multiple containers are running on the pod 
  kubectl logs -f <pod-name> <container-name>


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Application Lifecycle
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Rollout
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

For rolling out the deployment
  > kubectl rollout status deployment/myapp-deployment
for Rollout History
  > kubectl rollout histroy deployment/myapp-deployment

Deployment Stratergy
  1. Recreate Stratergy
     Destroy the current pods and apply new one 
  2. Rolling Update (default)
     Destroy and recreate one by one 

Triggering a Deployment
  1. Change the Manifest file 
  2. Refresh the pods using the apply commands
        kubectl apply -f deploy-definition.yaml
                or 
     trigger the deployment using command line without changing the manifest file
        kubectl set image deployment/myapp-deployment nginix=nginx:1.9.1
     (The changes done via command line will not update the Manifest file)

Deployment process:

Create Deployment:
  
  kubectl create -f deployment-definition.yaml

  For the initial application deployment, K8S creates ReplicaSet and deploy the pods

Get Deployment
  kubectl get deployments

Update Deployment
  kubectl apply -f deploy-definition.yaml
  kubectl set image deployment/myapp-deployment nginix=nginx:1.9.1

 For the application update, K8S will create new replica set with the new pods and 
     destoyed the old pods.
     All the replicasets (old and new) will still exist in the K8S cluseter 

Status
  kubectl rollout status deployment/myapp-deployment 
  kubectl rollout history deployment/myapp-deployment

Undoing the Deployment
  Kubectl rollout undo deployment/myapp-deployment
  
  The undo deployment will kill the pods from the new replicaset and bring the pods from the 
  older replicaset.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Configure Applications
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Configuring applications comprises of understanding the following concepts:

Configuring Command and Arguments on applications

Configuring Environment Variables

Configuring Secrets

We will see these next

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Docker Commands for application configuration
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

FROM  ubuntu    -> Takes the base image

CMD  sleep 5  or CMD  ["sleep" "5"]   Hardcoding a command for the container runtime

However by specifying a new command you can override the default command.

ENTRYPOINT ["sleep"]  -> Takes this command as entry point while running the container; however we
                         need to specify the time for the sleep

ENTRYPOINT ["sleep"] |  Using both of them together can solve this problems
CMD ["5"]            |  In this case if user doesn't enter the parameter it will be 5.

We can override the ENTRYPOINT point by specifying the same in the command line

docker run --entrypoint sleep2.0 ubuntu-sleeper 10

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Pod arguments related to Docker pararmeter
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
ENTRYPOINT ["sleep"]    ->   command: ["sleep2.0"]
CMD["5"]                ->   args: ["10"]












 



    
    

    




+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


















   

    


    

    

            








     

    


