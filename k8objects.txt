Format of pod manifest

File_name:  pod1.yaml

kind: Pod                       #kind is "Pod" for creating pod                 
apiVersion: v1                  #Api version for interacting with the K8S             
metadata:                       #Metadata for the pod               
  name: testpod                 #Name of the pod       
spec:                                    
  containers:                      
    - name: c00                 #Container name                
      image: ubuntu             #Image name            
      command: ["/bin/bash", "-c", "while true; do echo Hello Sanjay; sleep 5 ; done"]
  restartPolicy: Never          # Defaults to Always

Annotations
- Annotations is used for adding descriptions to the pod
- Annotations are defined in the metadata section 
- specified in the manifest
- Example
--------------------------------------------------------------------------------------------------------
kind: Pod                              
apiVersion: v1                     
metadata:                           
  name: testpod  
  annotations:
    description: This is first K8S pod (annotations)                
spec:                                    
  containers:                      
    - name: c00                     
      image: ubuntu              
      command: ["/bin/bash", "-c", "while true; do echo Hello Sanjay; sleep 5 ; done"]
  restartPolicy: Never         # Defaults to Always
--------------------------------------------------------------------------------------------------------
Multicontainer Pod
--------------------------------------------------------------------------------------------------------
- Multiple containers can be created in a pod; however, it is recommended to create one container/pod
- Even for mulitple containers per pod, one ip address will be assigned to the pod. 
- Internal communication between the containers can be done using "http://localhost:<port-no>" command
- refer to the "multicontainerPod.yaml" manifest for example

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Defining Environment Variables in pods
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- it is used to set some Environment variables for the pod
- useful for activities like passing secrets to the pod
- For pods, environement variables are defined in the spec section with "env:" keyword followed by key value pair.
- We can define any number of environement variables
- Example for the same is in "EnvVarPod.yaml" maintest

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Defining open port on the pods
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- We can open ports in the spec section  with keyword "- containerPort:<port-no>"
- Useful for opening the ports for required applications 
- We will try with example of Tomcat Server by opening port no 80

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Labels and Selectors
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- Labels are the mechanisms you use to Organize Kubernetes objects
- A label is a key-value pair without any predefined meaning that can be attached to a object
- Labels are similar to tags in AWS or git where you use a name for quick reference.
- You are free to choose labels as you need it to refer an environement which is used for dev or testing 
  or production, refer a product group like Department A, Department B.

- Multiple Lables can be given to a single object
- Labels are defined in the metadata section of the manifest
- example is given in the podTomcat.yaml and podFirst.yaml manifest
- kubectl commands for Labels 
    - labels can be seen using below commands
        - kubectl get pods --show-labels 
    - adding new labels to an existing pod
        - kubectl label pods tomcat cloud=false
    - listing pods matching a label
        - kubectl get pods -l env=dev  
    - listing condition can be defined as not equal as well 
        - kubectl get pods -l env!=dev
    - other operations also can be performed based on list condition
        - kubectl delete pod -l env!=dev   
        above command can delete multiple containers.
- Unlike name/UIDs, lables do not provide uniqueness, as in general, 
  we can expect many objects to carry the same label

- Once labels are attached to an object, we would need filters to narrow down
  these are called label selectors.
- The api currently supports two types of selectors 
    - equality based (= and !=)
        example env!=prod
    - set based (in, notin and exists)
        'env in (dev,prod,stage)'
        'env notin (dev,stage)'
- A label selector can be made of multiple requirements which are comma seperated
- mutiple values also can be matched together
    - kubectl get pods -l env=dev, app=tomcat, cloud=true 

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Node Selector (a use case for Label selector)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
One use case for Label Selector can be is to constrain the set of nodes on to which a 
pod can be scheduled. 
 - Generally such constraints are not required because the scheduler will do the 
   reasonable placement on the cluster; however in some cases we might need it.
 - we can use labels to tag nodes.
 - Once the nodes are tagged, you can use the label selectors to specify the pod to 
   run only at specific nodes.

 - node selector is defined in the spec section with "nodeSelector:" keyword
 - The object (pod) will not be created until the condition is met on the cluster.
 - kubectl get pods commands will show the status as pending.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Scaling and Replication
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- Kubernetes was designed to orchestrate multiple containers and replication
- Need for multiple containers/replication helps us with these.
- Reliablity: It help in getting high reliablity; in case any container fails, application will not
  get down.
- Load Balancing: Having mulitple versions of a containers enables you to easily send traffic to different
  instances to prevent overloading of a single instance or node.
- Scaling : When load does become too much for the number of existing instances,
  Kubernetes enables you to easily scale up/down your application, adding additional instances
  as needed.
- Rolling Updates: Updates to a service by replacing podes one by one.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Replication Controller (rc) 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- A replication controller is a object that enables you to easily create multiple pods on request, then make sure
  that number of pods always exist.
- If a pod created using RC will be automatically replaced in case of crash/fail or termination action.
- RC is recommended if you just want to make sure that fix number of pods are always running, 
  even after reboot of the instance.
- In case you create RC with 1 replica & the RC will make sure that one pod is always 
  running.
- While scaling down K8S delete the most recent containers 
- Please refer to the "rcUbuntu.yaml" for Replication Controller Manifest example

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Replication Set (rs) 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- A replica Set is next generation Replication Controller. 
- replication Controller only supports equality based selector, whereas the replica set supports set-based 
  selector
- replicaset rather than replication controller is used by other objects like Deployment
- difference in the manifest
    Replication Controller                                     replicaset
  kind: ReplicationController                           kind: Replicaset
  apiVersion: v1                                        apiVersion: apps/v1
  selector:                                             selector:
    env: dev                                            env in (dev, staging, prod)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Difference b/w replicaset and replication Controller
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
1. Replicase Set have a "selector" section in the spec section which allows replicaset to manage
   existing running pods.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Ways to Scale the pods in the cluster using replicaset
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
1. change the manifest file and use replace command:
   kubectl replace -f replicaset-definition.yaml 
2. by defining replicas in the kubectl command
   kubectl scale --replicas=6 -f replicaset-definition.yaml 
                    or
   kubectl scale --replicas=6 replicaset myapp-replicaset
                              --type---- ----name--------

  The option 2 will not increase the number of replicas in the manifest file.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Commands:
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Create the replicaset
  kubectl create -f replicaset-definition.yaml
get the replicasets from the cluster
  kubectl get replicaset
delete the replicaset
  kubectl delete replicaset myapp-replicaset  (this will delete all underlying pods)
scale the replicaset pods
   kubectl scale --replicas=6 -f replicaset-definition.yaml 
                    or
   kubectl scale --replicas=6 replicaset myapp-replicaset
                              --type---- ----name--------
Editing the existing running replicaset
   Kubectl edit replicaset myapp-replicaset 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Using Run Command to create the yaml files
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Kubernetes (K8s) Deployment Objects
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Replication Controller and replicaset is not able to do the updates and rollback apps in the cluster.
- A deployment object acts as a supervisor for pods, giving you fine grained control over how and when a new
  pod is rolled out, updated or rolled back to a previous state.
- When using deployment object, we first define the state of the app, then K8s cluster schedules mentioned app
  instance onto specific individual nodes.
- A deployment provides declarative updates for pods & replicaset.
- K8s then monitors, if the node hosting an instance goes down or pod is deleted; the deployment controller 
  replaces it.
- This provides a self-healing mechanism to address machine failure or maintenance.
- Below are the typical use cases of deployment:
    1. Create a deployment to rollout a replicaset 
       The replicaset creates pods in the background and checks the status of the rollout to see if it succeeds or not.
    2. Declare new state of the pods -> 
       By updating the podTemplateSpec of the deployment, a new replicaset is created and the deployment manages moving the pods from the old replicaset to the new
       one at a controlled rate. Each new replicaset updates the revision of the deployment.
    3. Rollback to an earlier deployment revision. If the current state of the deployment is not stable, each rollback updates
       the revision of the deployment.
    4. Scale up the deployment to facilitates more load
    5. Pause the deployment to apply multiple fixes to its podTemplateSpec and then resume it to start a new rollout.
    6. Cleanup older replicasets that you don't need anymore.
- If there are problems in the deployment, Kubernetes will automatically rollback to the previous version; however, you can also 
  explicitely rollback to a specific revision, as in our case to revision 1 (the original pod version)
- You can rollback to a specific version by specifying it with --to-revision

for example: kubectl rollout undo deploy/mydeployments --to-revision=2
Note: The name of the replicaset is always formatted as [deployment-name]-[random-string]

"kubectl get deploy" command can be used to get the deployments.
While inspecting the deployments, we can see the below fields: 
NAME - List the name of the deployments in the namespace.
READY - Display how many replicas of the application are available to your users. If follows the pattern ready/Desired.
UP-TO-DATE - Display the number of replicas that have been updated to achive the desired state.
AVAILABLE - Displays how many replicas of the application are available to your users.
AGE - Displays the amount of the time that the application has been running.

Reasons for failed deployments:
Your deployment may get stuck trying to deploy its newest replicaset without ever completing.
This can occur due to some of the following factors:
1. Insufficient Quota
2. Readiness Proble Failure
3. Image pull errors
4. Insufficient permissions
5. Limit Ranges
6. Application runtime misconfiguration




+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Service Object
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Service object allows to provision communication between differnt networks on the Kubernetes.

Spec Section of the Service Definition file is different:

spec:
    type: NodePort
    ports: 
     - targetPort: 80
       port: 80
       nodePort:30008
* only port is the mandatory field for service definition; others will be assumed as under:
  - NodePort will be allocated based on availablity from 30000-32767
  - targetPort will be assumed as port

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Differenece b/w Create and apply command
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Create command is an imperative command for applying changes to the cluster.

However, Apply command is declarative method of applying the changes to the infrastructure configuration.
- It creates a json file for last applied configuration which is checked everytime before applying the 
  configuration to the cluster.
- For example, if there is a change in the local manifest file and the same change is not in live env
  and is not in the last applied configuration. The change will be applied.
- For removing some changes from the live cluster, the change should exist in the last applied configuration
  and should not exist in the local manifest file.
- We should not mix the declarative and imperative commands while working with K8S cluster.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Labels and Selectors
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Labels are used to identify the K8s objects based on certain criteria
  - Selector keyword is used to group the K8s Objects with Kubectl commands.
  - Labels and selectors are used for the connecting internally with in K8s as well
    e.g. Deployment and replicasets are connected with labels and selectors only.
         Replicaset and pods are connected in the same way

    commands for selecting the objects with labels
     kubectl get pods --selector env=dev | wc -l

     count the pods with particular labels
     kubectl get pods --selector env=dev | wc -l

     Removing header before displaying the pods
     kubectl get pods --selector env=dev --no-headers | wc -l

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Taints and Tolerations
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Taints:
 - It is a property applied on the nodes to restrict them for accepting pods with applied tolerations.
 - By default node is able to accept all kinds of pods
 - If we want pod to be able to create on certain node, we need to define toleration on the pod
 - there are three types of taints for the node:
    - NoSchedule - Don't schedule pod on the node 
    - PreferNoSchedule - Prefer not to schedule but there is no guarntee that pod will not be scheduled 
    - NoExecute - This is usually applied when already pods are running on this node. The effect will
                  kill the pods which don't have tolerations and pods with toleration will have no impact.

- command for applying taint on the node
    kubectl taint nodes node-name key=value:taint-effect

    kubectl taint nodes node1 app=blue:NoSchedule

- applying tolerations in the pods
  spec:
    tolerations:
    - key: "app"
      operator: "Equal"
      value: "blue"
      effect: "NoSchedule"
    THE VALUES FOR THE TOLERATIONS WILL BE IN DOUBLE QUOTES ("").

- Checking whether node is tainted or not
  kubectl describe node <node-name> | grep Taints 

- Normally master node is tainted with NoSchedule; however, it can be changed if required.
- Untainting the master node can be done with below command:
  kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule-
- Tolerations allow pod to be created on tainted node; however, a pod with toleration can be created on 
  non-tainted nodes. So toleration and tainting doesn't guarntee that a pod will be created on certain node only.
- Tolerations and tainting have nothing to do with security.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Node Selectors
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

- In case we want to place our node of some particular node, we can define the same using the below 
  approach:
  1. Assign a label to the node 
  2. Use the label in the Pod Definition File for selecting the node for provisioning the pod.

- Example:
Labelling the node with size=large
  Kubectl label nodes <node-name> <label-key>=<label-value>
  Kubectl label nodes node1 size=large

Assigning node selector in the pod definition file
  spec:
     nodeSelector: 
         size: large

This can solve the problem of provisioing the pod on some particular node; however, this can't solve the problem 
of provisioing the pod on complex conditions (Not or OR conditions) e.g. large or medium node/ not small 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Node Affinity
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
The complex conditions for the pod provisioing on the node can be handled using node affinity. 
- Node Affinity is assigned within the node specification file
e.g.

spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoreDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - large
            - medium

  other values for operator can be:
   notIn, Exists 

   Exists can be used in case the label only exists irrespective of their value.
   e.g. we have not provisioned small size nodes. So using Exist operator without values should place
        the pod on either of the nodes.

  Node Affinity Types:
  
  available Affinity types
  requiredDuringSchedulingIgnoredDuringExecution
  preferredDuringSchedulingIgnoredDuringExecution
  -------------------------------------------------------------------------------
  |                       | During Schduling  |  During Execution               |
  -------------------------------------------------------------------------------
  |    Type 1             |  Required         |   Ignored                       |
  -------------------------------------------------------------------------------
  |    Type 2             |  Preferred        |   Ignored                       |
  -------------------------------------------------------------------------------
  For the above two types the Node Affinity will be considered only while scheduling the pods.
  Not during Execution.

  The below is planned:
  requiredDuringSchedulingRequiredDuringExecution
  -------------------------------------------------------------------------------
  |                       | During Schduling  |  During Execution               |
  -------------------------------------------------------------------------------
  |    Type 3             |  Required         |   Required                      |
  -------------------------------------------------------------------------------
  In this case if the label is removed after scheduling the pod, the pod will be killed.

  For keeping the pods on required nodes, sometime a mixture of Node Affinity, taint and toleration both are 
  used in conjunction.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Resource Limits
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

By default, K8s looks for 0.5 cpu and 256 Mi of memory for a new pod creation request;however, this can be 
changed in the pod definition file as shown under:

spec:
  containers:
  - name: nginx
    image: nginx
    ports:
     - containerPort:8080
  resources:
    requests:
      memory: "1Gi"
      cpu: 1
    limit:
      memory:"2Gi"
      cpu: 2

similary by default, the kubernetes will limit the cpu & memory limits for the pod as 1 vcpu and 512 Mi; However, it can 
be defined in the pod definition file if you need over the default requirement.

**INCASE THE POD NEEDS MORE CPU THAN DEFINED, K8S WILL THROTTLE THE CPU. HOWEVER, POD MAY BE USING MORE MEMORY THAN Required
HOWEVER, IF IT HAPPENS FREQUENTLY, THE POD WILL BE TERMINATED.


                            Different units for the memory:           
1G (Gigabyte) = 1,000,000,000 bytes       | 1 Gi (Gibibyte) = 1,073,741,824 bytes
1M (Megabyte) = 1,000,000 bytes           | 1 Mi (Mebibyte) = 1,048,576 bytes
1K (Kilo)     = 1,000 bytes               | 1 Ki (Kibibyte) = 1,024 Bytes


When a pod is created the containers are assigned a default CPU request of .5 and memory of 256Mi". 
For the POD to pick up those defaults you must have first set those as default values for request and 
limit by creating a LimitRange in that namespace.

apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/

apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/

https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource
  
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
DaemonSets
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- Daemon set is a K8S objects used for scheduling pod on each node.
- Use Cases:
  1. Installing Moniotring Solution of each node
  2. Implementing Networking Solutions
  3. Installing Kube Proxy (a required Element on the Node)
- Daemonsets are similar to ReplicaSets in K8S
- Daemonsets create a pod on each node as the node is created in the network.
- How to implement the creation of DaemonPods on each node.
    - Before Kubernetes v1.12, the pod were created with the nodeName. So the pod land on the required node.
    - After v1.12, session affinity and default scheduler is used for creating pods on the nodes.

Steps for creating the Daemonsets
1. Get the YAML file for creating the Deployment using the below command:
   kubectl create deployment <deplyment-name> --image=<image-name> -n <name-space-name> --dry-run=client
   -o yaml > Daemon.yaml

2. Remove the below configurations from the yaml file
    - status
    - Stratergy
    - replicas
   replace the below configuration from the yaml file
    - deplyment keyword with DaemonSet keyword

3. Run the the manifest
  
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Static Pods
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Finding the Static Pod path for the node

1. Find the config file
   ps -aux | grep kubelet
   search --config (--config=/var/lib/kubelet/config.yaml)
2. Find the "staticPodPath" in the config file.
   staticPodPath: /etc/kubernetes/manifests

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Mulitple Schedulers
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Multiple Schedulers can be run on the Kubernetes Master node by creating a copy of the 
default scheduler or creating your own.
Steps for spinning the additional Scheduler are as under:
1. Copy the /etc/kubernetes/manifests/kube-scheduler.yaml file to new-name
2. open the file in a editor
3. add scheduler name in the command section
4. define lock-object=<scheduler-name>
5. Create the pod using Kubectl Create Command

Viewing Schedulers
-------------------
kubectl get pods -n kube-system

The new scheduler should appear here and should be in running status.

Assigning scheduler to the pod
------------------------------
The pod definition file will have new field named schedulerName: and the new scheduler name.


Vieving events
---------------
kubectl get events -o wide

This command should show which scheduler picked up the pod creation with Source of the event.

Kubectl logs <custom-scheduler-name> --namespace=kube-system

The above command will show the logs for the scheduler.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Monitoring the Cluster
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
K8S cluster can be monitored using below tools for cpu and memory usage:
1. Metrics Server   - In memory monitoring solution and can't be used for historical 
                      performance analysis.
2. Prometheus  
3. Elastic stack
4. Datadog 
5. Dynatrace 

* one of the earlies project for the K8S monitoring was HEAPSTER; however, it is not deprecated

Metric Server is one of the good options available now. 
- You can have one meteric server per K8S cluster
- It is an Inmemory store
- Performance Data is collected from Kubelet (aAdvisor) 

For Minikube use the below command to enable the Metrics-server
minikube addons enable metrics-server

For all others
Need to clone the repo from github for meterics server and creating the required components.

Common Commands for the Performance Check:

kubectl top node
kubectl top pod

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Kubernetes Logging
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Logs for the Kubernetes pods are inspected using the below command
when one container per pod 
  kubectl logs -f <pod-name>
When multiple containers are running on the pod 
  kubectl logs -f <pod-name> <container-name>


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Application Lifecycle
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Rollout
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

For rolling out the deployment
  > kubectl rollout status deployment/myapp-deployment
for Rollout History
  > kubectl rollout histroy deployment/myapp-deployment

Deployment Stratergy
  1. Recreate Stratergy
     Destroy the current pods and apply new one 
  2. Rolling Update (default)
     Destroy and recreate one by one 

Triggering a Deployment
  1. Change the Manifest file 
  2. Refresh the pods using the apply commands
        kubectl apply -f deploy-definition.yaml
                or 
     trigger the deployment using command line without changing the manifest file
        kubectl set image deployment/myapp-deployment nginix=nginx:1.9.1
     (The changes done via command line will not update the Manifest file)

Deployment process:

Create Deployment:
  
  kubectl create -f deployment-definition.yaml

  For the initial application deployment, K8S creates ReplicaSet and deploy the pods

Get Deployment
  kubectl get deployments

Update Deployment
  kubectl apply -f deploy-definition.yaml
  kubectl set image deployment/myapp-deployment nginix=nginx:1.9.1

 For the application update, K8S will create new replica set with the new pods and 
     destoyed the old pods.
     All the replicasets (old and new) will still exist in the K8S cluseter 

Status
  kubectl rollout status deployment/myapp-deployment 
  kubectl rollout history deployment/myapp-deployment

Undoing the Deployment
  Kubectl rollout undo deployment/myapp-deployment
  
  The undo deployment will kill the pods from the new replicaset and bring the pods from the 
  older replicaset.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Configure Applications
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Configuring applications comprises of understanding the following concepts:

Configuring Command and Arguments on applications

Configuring Environment Variables

Configuring Secrets

We will see these next

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Docker Commands for application configuration
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

FROM  ubuntu    -> Takes the base image

CMD  sleep 5  or CMD  ["sleep" "5"]   Hardcoding a command for the container runtime

However by specifying a new command you can override the default command.

ENTRYPOINT ["sleep"]  -> Takes this command as entry point while running the container; however we
                         need to specify the time for the sleep

ENTRYPOINT ["sleep"] |  Using both of them together can solve this problems
CMD ["5"]            |  In this case if user doesn't enter the parameter it will be 5.

We can override the ENTRYPOINT point by specifying the same in the command line

docker run --entrypoint sleep2.0 ubuntu-sleeper 10

If both CMD and ENTRYPOINT is defined in Dockerfile. The CMD will execute.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Pod arguments related to Docker pararmeter
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
ENTRYPOINT ["sleep"]    ->   command: ["sleep2.0"]
CMD["5"]                ->   args: ["10"]

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Defining environement variables in pod definition file
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
spec:
  containers:
  - name: 
    image:
    ports:
    env:
      - name: APP_COLOR
        value: blue
      - name: APP
        value: flask

 Other alternative ways of passing environement variables to the pod are as under:
 - ConfigMap
   ConfigMaps are used to pass the configuration data in the form of key value pairs. It includes two step 
   process
    1. Create the ConfigMap
    2. Inject the values from the ConfigMap into the pod 
 
 - Creating ConfigMap objects:
   - imperative way:
     kubectl create configMap 
      <configMap-Name> --from-literal=<key>=<value>
                       --from-literal=<key2>=<value2> and so on.
     kubectl create configMap app-config --from-literal=APP_COLOR=blue 

     also

     kubectl create configMap app-config --from-file=<path-to-the-file>

     kubectl create configMap app-config --from-file=app_config.properties

  - declarative approach
      create the ConfigMap configuration file
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: app-config
          data:
            APP_COLOR: blue
            APP_MODE: prod

      run the create command
        kubectl create -f configMap.yaml 
    
    We can create as many as we need ConfigMaps
    for example: app-config, mysql-config, redis-config 

  Listing the ConfigMaps
    kubectl get ConfigMaps

  describing the ConfigMaps 
  kubectl describe ConfigMaps

Injecting the configmap values in the Pods

  The keyvalues defined in the configMaps can be injected in the pod-definition file as under:
  apiVersion:
  kind:
  metadata
  spec:
    containers:
    - name:
      image:
      ports:
      envFrom:
        - configMapRef:
            name: app-config 
        - configMapRef:
            name: rds-config 
    Creating the pod will inject the values inside the pod in the form of env variables
|-------------------------------------------|
| Replacing a pod with updated definition   |
|-------------------------------------------|
|    kubectl replace --force -f pod.yaml    |
|-------------------------------------------|

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Configuring Secrets in the K8S
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
spec:
  containers:
  - name: 
    image:
    ports:
    env:
      - name: APP_COLOR
        value: blue
      - name: APP
        value: flask

 Secrets are used for passing the confidential parameters to the pod. It is used in the form of key value pairs. 
 It includes two step process
 Creating the Secrets:
    1. Create the Secrets 
    2. Inject the values from the secrets into the pod 
 
 - Creating Secret objects:
   - imperative way:
     kubectl create secret generic
      <secret-Name> --from-literal=<key>=<value>
                       --from-literal=<key2>=<value2> and so on.
     kubectl create secret generic app-secret --from-literal=DB_HOST=mysql 

     also

     kubectl create secret generic app-secret --from-file=<path-to-the-file>

     kubectl create secret generic app-secret --from-file=app_config.properties

  - declarative approach
      create the Secret configuration file
          apiVersion: v1
          kind: Secret 
          metadata:
            name: app-secret 
          data:
            DB_HOST:      Hash Value for the mysql
            DB_USER:      Hash Value for the root
            DB_PASSWORD:  Hash Value for the paswrd

      run the create command
        kubectl create -f .yaml     
|-----------------------------------------------------------------|
|         Converting Hash Value                                   |
|-----------------------------------------------------------------|
|         Encodeing the Value                                     |
|-----------------------------------------------------------------|
|    sanjay@sanjay01:~$ echo sanjay | base64                      |
|    c2FuamF5Cg==                                                 |
|-----------------------------------------------------------------|
|         Decoding the Value                                      |
|-----------------------------------------------------------------|
|    sanjay@sanjay01:~$ echo c2FuamF5Cg==  | base64  --decode     |
|    sanjay                                                       |
|-----------------------------------------------------------------|
   
    We can create as many secrets we want.
    for example: app-secret, mysql-secret, redis-secret

  Listing the secrets
    kubectl get secrets 

  describing the secret
  kubectl describe secret 

  Checking values of Secret in base64

Injecting the configmap values in the Pods

  The key values defined in the secrets can be injected in the pod-definition file as under:
  apiVersion:
  kind:
  metadata
  spec:
    containers:
    - name:
      image:
      ports:
      envFrom:
        - secretRef:
            name: app-secret 
        - secretRef:
            name: rds-secret
    Creating the pod will inject the values inside the pod in the form of env variables

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
A note about Secrets!
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. 
As such the secrets can be considered as not very safe. The concept of safety of the Secrets is a bit confusing in Kubernetes. 
The kubernetes documentation page and a lot of blogs out there refer to secrets as a "safer option" to store sensitive data. 
They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data. 
n my opinion it's not the secret itself that is safe, it is the practices around it. 
Secrets are not encrypted, so it is not safer in that sense. However, some best practices around 
using secrets make it safer. As in best practices like:
  - Not checking-in secret object definition files to source code repositories.
  - Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD. 
  - Also the way kubernetes handles secrets. Such as:
    - A secret is only sent to a node if a pod on that node requires it.
    - Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.
    - Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.
  Read about the protections and risks of using secrets here
    - Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, 
      such as using tools like Helm Secrets, HashiCorp Vault. I hope to make a lecture on these in the future.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
InitContainers
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
In a multi-container pod, each container is expected to run a process that stays alive as long as the POD's lifecycle. 
For example in the multi-container pod that we talked about earlier that has a web application and logging agent, both 
the containers are expected to stay alive at all times. The process running in the log agent container is expected to stay 
alive as long as the web application is running. If any of them fails, the POD restarts.

But at times you may want to run a process that runs to completion in a container. For example a process that pulls a code 
or binary from a repository that will be used by the main web application. 
That is a task that will be run only  one time when the pod is first created. Or a process that waits 
for an external service or database to be up before the actual application starts. That's where initContainers comes in.

An initContainer is configured in a pod like all other containers, except that it is specified inside a initContainers 
section,  like this:

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ; done;']


When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts. 

You can configure multiple such initContainers as well, like how we did for multi-pod containers. In that case each init container is run one at a time in sequential order.

If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']


Read more about initContainers here. And try out the upcoming practice test.

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
CLUSTER MAINTENANCE
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
pod-eviction-timeout - 
    Time defined in the Kube-controller-manager for waiting for node to come up online before declaring the node dead.

Before performing maintenance on the node, we need to tranfer the running pods to other nodes
  - kubectl drain node-1 
      this command will gracefully terminate the pods running on the node-1 and mark the node unschedulebale.
      kubectl describe node node01 will show unschedulebale = true. 
  - kubectl uncordon node-1
      After the node maintenance is completed, above command needs to run to make the node available for pods.
  - kubectl cordon node-1
      This command will make the node not available for pod scheduling; however, it won't terminate the existing pods.

kubectl drain <node-name> command will not run if the pod running is not part of the repicaset, daemonset or deployment.
Incase such a case the pod will be lost forever.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
CLUSTER SECURITY
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Types of users in K8s cluster:
1. administer
2. Developers
3. 3rd Party Apps for intergration of applications
4. End users

***End Users' security is taken care by the application. 

1. K8S does not manage user accounts natively and depend on 3rd party servers for the same. like
   LDAP, IAM etc. So kubectl create user <user-name> and kubectl list users are not valid commands in K8S.
2. However, K8S natively support service Accounts. Service accounts are used for providing access to 3rd Party apps.

So the user authentication requests will go to kube-apiserver and api-server can user either of the below
methods to validate the request:
  1. statics password file
  2. Static token file
  3. Certificates
  4. Identity Services like LDAP and IAM

Using the Static Password File:
-------------------------------
Password file usually have three fields in csv file:
password,user, userid, group(optional) 

** The file name will be passed as an option to the kubeapi server:
using the apiserver as an service:
  ExecStart=/usr/local/bin/kube-apiserver \\
  --basic-auth-file=user-details.csv 
                  or
  using the kubeadm tool:
  - modify the pod definition file to include the file name.
  - kubeadm tool will restart the service automatically after the defition update
  apiVersion: v1
  kind: Pod
  metadata:
  spec:
    containers:
     - command:
       -kube-apiserver
       - --basic-auth-file=user-details.csv 
       image: 
       name: 
  
  User request for the authentication:

  curl -v -k https://master-node-ip:6443/api/v1/pods -u "user1:passwords123"
  {
      "kind": "PodList", 

       request details
  }

  ApiServer will authenticate the details against the file provided and then execute the request.

using static token file
------------------------
Token for the user authentication are provided in a file usually have three fields in csv file:
authenticationToken,user, userid, group(optional) 

** The file name will be passed as an option to the kubeapi server:
using the apiserver as an service:
  ExecStart=/usr/local/bin/kube-apiserver \\
  --basic-auth-file=user-details.csv 
                  or
  using the kubeadm tool:
  - modify the pod definition file to include the file name.
  - kubeadm tool will restart the service automatically after the defition update
  apiVersion: v1
  kind: Pod
  metadata:
  spec:
    containers:
     - command:
       -kube-apiserver
       - --basic-auth-file=user-details.csv 
       image: 
       name: 
  
  User request for the authentication:

  curl -v -k https://master-node-ip:6443/api/v1/pods --header "Authorization: Bearer token-details"
  {
      "kind": "PodList", 

       request details
  }

  ApiServer will authenticate the details against the file provided and then execute the request.

***THE ABOVE AUTHENTICATION APPROACHES STORES THE TOKEN IN PLAIN TEXT FORMAT & ARE NOT THE RECOMMENDED
   APPROACH FOR AUTHENTICATION.
  
  
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Container Storage Interface
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
The Container Storage Interface (CSI) is a standard for exposing arbitrary block and file storage 
systems to containerized workloads on Container Orchestration Systems (COs) like Kubernetes. Using 
CSI third-party storage providers can write and deploy plugins exposing new storage systems in 
Kubernetes without ever having to touch the core Kubernetes code.
The target audience for this site is third-party developers interested in developing CSI drivers 
for Kubernetes. Kubernetes users interested in how to deploy or manage an existing CSI driver on 
Kubernetes should look at the documentation provided by the author of the CSI driver.

In other words, the Kubernetes should be able to call the RPCs for creating a new volume deleting 
and existing volume and place a workload that uses volume onto a node.
The volume driver needs to implement the programs to support the RPC.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Defining Volume for Kubernetes pods in Kubernets nodes
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
apiVersion: v1
kind: Pod
metadata:
spec:
  containers:
  - image: alpine
    name:
    command: [ ]
    args: [ ]
    volumeMounts:
    - mountPath: /opt
      name: data-volume
  
  volumes:
  - name: data-volume
    hostpath:
      path: /data
      type: directory

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Defining Volume for Kubernetes pods in AWS Block Store (EBS)
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
apiVersion: v1
kind: Pod
metadata:
spec:
  containers:
  - image: alpine
    name:
    command: [ ]
    args: [ ]
    volumeMounts:
    - mountPath: /opt
      name: data-volume
  
  volumes:
  - name: data-volume
    AWSElasticBlockStore:
      volumeId: <volumeID>
      type: ext4
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
PERSISTENT VOLUMES 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Persistent Volumes are clusterwide stoage and can be configured to allocate storage to different 
pods based on their needs.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Defining PV for Kubernetes pods using AWS Block Store (EBS)
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
apiVersion: v1
kind: PersistentVolume
metadata:
  name:pv-vol1
spec:
  accessModes:
    -ReadWriteOnce
  capacity:
    storage: 1Gi 
  AWSElasticBlockStore:
    volumeID: <volumeID>
    fsType: ext4

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Defining PV for Kubernetes pods using HostPath
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
apiVersion: v1
kind: PersistentVolume
metadata:
  name:pv-vol1
spec:
  accessModes:
    -ReadWriteOnce
  capacity:
    storage: 1Gi 
  hostPath:
    path: /tmp/data
  persistentvolumeReclaimPolicy: Retain

Commands for PV

    kubectl create -f pv-definition.yaml
    kubectl get persistentvolume
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Defining PV Claim 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

- PV and PV claim are two different objects
- PVC is created by user and pv is created by administer
- Once the PVC is created, K8S binds the PV with the PVC
- Each PVC is bound by one PV
- If mulitple pvs are available, Labels and Selector can be used to select PV.
- Below is the criteria for binding the claim with the PVC
  - Sufficient capacity
  - Access Modes 
  - Volume Modes 
  - Storage Class 
  - Selector 
  If PVC is created and no PV is available, the PVC will stay in pending state until new volume is 
  not created.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Defining PVC
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name:myclaim
spec:
  accessModes:
    -ReadWriteOnce
  resources:
    requests: 
      storage: 500Mi  
    


Commands for PV

    kubectl create -f pvc-definition.yaml
    kubectl get persistentvolumeClaims

Deleting the PVC
  kubectl delete persistentvolumeClaim myclaim 
  persistentvolumeclaim "myclaim" is deleted.

  after the PVC is deleted, the PV status will depend on persistentvolumeReclaimPolicy. 
    - Bydefault it will be "Retain" but it will not be allocated any other PVC
    - it can be set to "Delete" if needed so.
    - it can be "Recycle" where the data will be scrub before it is made available to other PVC.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Using PVCs in PODs
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under 
persistentVolumeClaim section in the volumes section like this:

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim

The same is true for ReplicaSets or Deployments. Add this to the pod template section of a 
Deployment on ReplicaSet.

Reference URL: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes











   

    


    

    

            








     

    


